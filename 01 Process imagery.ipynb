{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd8a34e-c04c-4660-84bc-c1057ed9ec16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Raster acquisition, processing and analysis with Databricks\n",
    "\n",
    "Continuing from the previous notebook, this notebook will demonstrate how to:\n",
    "- Read the freshly downloaded imagery into a Spark Dataframe with Mosaic, reproject each raster and collate them into multiband files;\n",
    "- Join this imagery dataset to the areas of interest and use the AoI geometries to clip the rasters, retaining only the pixels that fall inside each AoI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469a50a3-a5d9-4094-a211-0fa60c8dbc09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install the libraries and prepare the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0e3cc4-5baf-48ad-be02-7d3b5c82ebd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For this demo we will require a few spatial libraries that can be easily installed via pip install. We will be using gdal, rasterio, pystac and databricks-mosaic for data download and data manipulation. We will use planetary computer as the source of the raster data for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a123b1c-fce9-468c-b100-79235d95f492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "project_path = os.path.dirname(notebook_path)\n",
    "os.environ[\"PROJECTCWD\"] = project_path\n",
    "\n",
    "%pip install /Workspace$PROJECTCWD/databricks_mosaic-0.4.3-py3-none-any.whl\n",
    "%pip install --quiet rasterio==1.3.5 gdal==3.4.1 pystac pystac_client planetary_computer tenacity rich osdatahub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c915a766-8989-49c2-bd1d-eca453d6f2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33de043-55b7-4fe0-9356-56df486d963e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import library\n",
    "import mosaic as mos\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "data_product = \"OpenGreenspace\"\n",
    "\n",
    "current_user = spark.sql(\"select current_user() as user\").first()[\"user\"]\n",
    "data_root = f\"/tmp/{current_user}/{data_product}/data\"\n",
    "output_path = data_root.replace(\"/data\", \"/outputs\")\n",
    "\n",
    "dbutils.fs.mkdirs(data_root)\n",
    "dbutils.fs.mkdirs(output_path)\n",
    "\n",
    "os.environ[\"DATADIR\"] = f\"/dbfs{data_root}\"\n",
    "os.environ[\"OUTDIR\"] = f\"/dbfs{output_path}\"\n",
    "\n",
    "CATALOG = \"your_catalog\"\n",
    "SCHEMA = \"your_schema\"\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59192d9-2d44-4bb6-ad6b-dc3e4d92634b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n",
    "mos.enable_mosaic(spark, dbutils)\n",
    "mos.enable_gdal(spark, with_checkpoint_path=f\"/dbfs{output_path}/checkpoint/{datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f36c7f77-08f7-418c-a322-e66957acc042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Catalogue the imagery with Mosaic\n",
    "\n",
    "Since we have the direct paths where the data has been downloaded, we can use Mosaic's `rst_fromfile()` method to create pointers within a Spark dataframe to the source files.\n",
    "\n",
    "If this wasn't the case (say, for example, we just had one big folder full of imagery), we could instead use the `GDAL` Spark Data Source in Mosaic[↗︎](https://databrickslabs.github.io/mosaic/api/raster-format-readers.html#spark-read-format-gdal) to achieve the same outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97db5c63-ef7a-434a-bf99-28086d75ab50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "imagery_root = f\"/tmp/{current_user}/{data_product}/imagery\"\n",
    "dbutils.fs.mkdirs(imagery_root)\n",
    "\n",
    "imagery_table_ref = f\"{CATALOG}.{SCHEMA}.imagery\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30221e84-3842-4e8b-9edc-b7ac0a5acad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(imagery_table_ref).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eda6302-1bb4-4a09-9898-64f4f400ce7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here we check if any files are corrupt and/or can't be opened by GDAL, and remove them for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920d1b6f-53b1-466c-a313-6436ade3c1a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from osgeo import gdal\n",
    "\n",
    "directory = f\"/dbfs/tmp/{current_user}/{data_product}/imagery\"\n",
    "files = os.listdir(directory)\n",
    "\n",
    "for filename in tqdm(files):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    try:\n",
    "        dataset = gdal.Open(filepath)\n",
    "        if dataset:\n",
    "            driver = dataset.GetDriver().ShortName\n",
    "        else:\n",
    "            print(f\"File: {filename}, could not be opened by GDAL.\")\n",
    "            dbutils.fs.rm(f\"dbfs:/tmp/{current_user}/{data_product}/imagery\" + filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening file {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3665a6c7-eaee-46be-931c-0794f521b1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_files = (spark.createDataFrame(dbutils.fs.ls(f\"dbfs:/tmp/{current_user}/{data_product}/imagery\"))\n",
    "            .withColumn(\"downloaded_path\", F.expr(\"replace(path, 'dbfs:', '/dbfs')\"))\n",
    "            .withColumn(\"present\", F.lit(1))\n",
    "            .select(\"downloaded_path\", \"present\")\n",
    ")\n",
    "df_t = (spark.table(imagery_table_ref).alias(\"r\").join(df_files.alias(\"listing\"), on=\"downloaded_path\", how=\"left\")\n",
    "        .filter(\"present IS NOT NULL\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649cb6b7-b82d-4556-865c-382948f76506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rasters_raw = (\n",
    "  df_t\n",
    "  .repartition(sc.defaultParallelism * 10)\n",
    "  .withColumn(\"tile\", mos.rst_fromfile(F.col(\"downloaded_path\")))\n",
    "  )\n",
    "\n",
    "\n",
    "rasters_raw.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb0da1f-6a35-4d25-8a9e-3f672171ed70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reproject the rasters into WGS84\n",
    "In preparation for joining this dataset with the Areas of Interest data, we need to reproject it into a common CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31bf547b-b4e4-44bc-bdd4-d3ddd49c5224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_raster_table_ref = f\"{CATALOG}.{SCHEMA}.transformed\"\n",
    "\n",
    "(\n",
    "  rasters_raw\n",
    "  .withColumn(\"original_projection\", mos.rst_srid(\"tile\"))\n",
    "  .withColumn(\"tile\", mos.rst_transform(\"tile\", F.lit(4326)))\n",
    "  .write\n",
    "  .mode(\"overwrite\")\n",
    "  .saveAsTable(transformed_raster_table_ref)\n",
    ")\n",
    "\n",
    "raster_4326 = spark.table(transformed_raster_table_ref)\n",
    "raster_4326.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562b9fd2-052d-4df4-8013-c5732b773143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raster_4326.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c39bd576-d9ba-443f-a052-216ee5b512e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can use the `last_error` metadata field to check if any of the processing failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1e6eaff-0220-4782-b2ea-c7fc0292a244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raster_4326.where(\"tile.metadata['last_error'] <> ''\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644fea84-c411-4262-9adf-5c145cad25ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's also check that we have at least one image for every part of the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "227b999b-6ed2-4035-b6c6-4ddef5d6f643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "coverage = (\n",
    "  raster_4326\n",
    "  .select(mos.rst_boundingbox(\"tile\").alias(\"bbox\"))\n",
    "  .distinct()\n",
    "  .groupBy()\n",
    "  .agg(mos.st_union_agg(\"bbox\").alias(\"wkb\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e779b0f4-82dd-4abf-bff2-e724216f79c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here's an example of the map the following command should produce.\n",
    "<img src='./assets/coverage-map.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "347240f7-9935-4288-bac7-96e1b5a25d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%mosaic_kepler\n",
    "coverage wkb geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0d5ca3-a885-4717-9066-85e7d2a229e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Assemble the single-band rasters into multiband rasters\n",
    "\n",
    "We'll use Mosaic's `rst_frombands()` method to collect our 12 individual raster bands into a single raster. The default behaviour is to upsample the bands to the resultion of the highest.\n",
    "\n",
    "In order to do this, we'll need to reshape our dataframe: representing each sweep's bands as columns single dataframe row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a58db3cc-aedf-4256-a821-757e0524fca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raster_4326.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ac2552-7dce-49d1-a70f-d61644509607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bands = [\n",
    "    \"B01\",\n",
    "    \"B02\",\n",
    "    \"B03\",\n",
    "    \"B04\",\n",
    "    \"B05\",\n",
    "    \"B06\",\n",
    "    \"B07\",\n",
    "    \"B08\",\n",
    "    \"B8A\",\n",
    "    \"B09\",\n",
    "    \"B11\",\n",
    "    \"B12\",\n",
    "]\n",
    "\n",
    "raster_multiband_table_ref = f\"{CATALOG}.{SCHEMA}.multiband\"\n",
    "\n",
    "filter_expr = \" OR \".join([f\"{b} IS NOT NULL\" for b in bands])\n",
    "\n",
    "(\n",
    "  raster_4326\n",
    "    .select(\"item_id\", \"datetime\", \"name\", \"tile\")\n",
    "    .groupBy(\"item_id\", \"datetime\")\n",
    "    .pivot(\"name\", bands)\n",
    "    .agg(F.first(\"tile\"))\n",
    "    .filter(filter_expr)\n",
    "    .withColumn(\"tile\", F.expr(\"try_sql(rst_frombands(ARRAY(B01, B02, B03, B04, B05, B06, B07, B08, B8A, B09, B11, B12)))\"))\n",
    "    .drop(*bands)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(raster_multiband_table_ref)\n",
    ")\n",
    "\n",
    "raster_multiband = spark.table(raster_multiband_table_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b7e6e0-1fd7-4a9a-a968-6073420fca71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raster_multiband.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5358479d-7b1d-46bf-9916-011b6c5db9d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Join our vector and raster datasets and clip the rasters\n",
    "\n",
    "In order to compute statistics for each area of interest, we need to create subsets of the raster pixels that correspond to each AoI's area.\n",
    "\n",
    "We can achieve this by\n",
    "  - joining the multiband raster data with the ID sets computed earlier;\n",
    "  - exploding these sets and looking up the corresponding geometries (so we have an image and geometry per ID / granule combination); then\n",
    "  - clipping the image by the geometry using the `rst_clip()` method.\n",
    "\n",
    "While we're doing this, we'll use standard Spark SQL functions to filter our imagery to only include the latest image available for each AoI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3f1e217-5a34-41a6-ad01-c6897340174d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aoi_type = \"Golf Course\"\n",
    "aoi_table_ref = f\"{CATALOG}.{SCHEMA}.aois\"\n",
    "\n",
    "aois = spark.table(aoi_table_ref)\n",
    "aois.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9195d41b-7e58-4e06-bcfb-950d27bb46bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "latest_raster_by_aoi_table_ref = f\"{CATALOG}.{SCHEMA}.latest_aoi_raster\"\n",
    "\n",
    "windowSpec = Window.partitionBy(\"id\").orderBy(F.desc(\"datetime\"))\n",
    "\n",
    "(\n",
    "  raster_multiband\n",
    "  .join(\n",
    "    other=(\n",
    "      raster_4326\n",
    "      .select(\"item_id\", \"ids\")\n",
    "      .distinct()\n",
    "    ),\n",
    "    how=\"inner\", \n",
    "    on=\"item_id\"\n",
    "  )\n",
    "  .withColumn(\"id\", F.explode(\"ids\"))\n",
    "  .withColumn(\"rank\", F.row_number().over(windowSpec))\n",
    "  .filter(\"rank = 1\")\n",
    "  .drop(\"rank\")\n",
    "  .select(\"item_id\", \"datetime\", \"id\", \"tile\")\n",
    "  .write\n",
    "  .mode(\"overwrite\")\n",
    "  .saveAsTable(latest_raster_by_aoi_table_ref)\n",
    ")\n",
    "\n",
    "raster_latest = spark.table(latest_raster_by_aoi_table_ref)\n",
    "raster_latest.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c198af5a-ab18-4f8b-be0f-55fc431086f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raster_latest.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20509817-1eb7-4d5f-b014-3efb0f5c896a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raster_latest.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "092c0276-48f2-4591-89cf-931a33f83870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clipped_image_table_ref = f\"{CATALOG}.{SCHEMA}.clipped_rasters\"\n",
    "\n",
    "(\n",
    "  raster_latest\n",
    "  .join(\n",
    "    other=aois,\n",
    "    how=\"inner\",\n",
    "    on=\"id\"\n",
    "  )\n",
    "  .withColumn(\"geometry_4326\", mos.st_aswkt(\"geometry_4326\"))\n",
    "  .withColumn(\"tile\", mos.rst_clip(\"tile.result\", \"geometry_4326\"))\n",
    "  # .withColumn(\"geometry_4326\", mos.st_aswkt(\"geometry_4326\"))\n",
    "  .write\n",
    "  .mode(\"overwrite\")\n",
    "  .saveAsTable(clipped_image_table_ref)\n",
    ")\n",
    "\n",
    "clipped_images = spark.table(clipped_image_table_ref)\n",
    "clipped_images.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da7d61f-9c6a-4e72-a2de-d38b35319d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clipped_images.count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3869932902612248,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01 Process imagery",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
